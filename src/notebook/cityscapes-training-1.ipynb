{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cdadc8",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision.io import decode_image\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from clearml import Task\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c6ee0",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    DATA_PATH: str = \"/home/hxastur/vscode-projects/cityscapes-segmentation/dataset\"\n",
    "    ANNOTATIONS_DATA_PATH: str = os.path.join(DATA_PATH, \"gtFine/gtFine\")\n",
    "    ANNOTATIONS_TRAIN_PATH: str = os.path.join(ANNOTATIONS_DATA_PATH, \"train\")\n",
    "    ANNOTATIONS_TEST_PATH: str = os.path.join(ANNOTATIONS_DATA_PATH, \"test\")\n",
    "    ANNOTATIONS_VAL_PATH: str = os.path.join(ANNOTATIONS_DATA_PATH, \"val\")\n",
    "    ANNOTATION_TYPES = [\"color.png\", \"instanceIds.png\", \"labelIds.png\", \"polygons.json\"]\n",
    "    IMAGE_DATA_PATH: str = os.path.join(DATA_PATH, \"left/leftImg8bit\")\n",
    "    IMAGE_TRAIN_PATH: str = os.path.join(IMAGE_DATA_PATH, \"train\")\n",
    "    IMAGE_TEST_PATH: str = os.path.join(IMAGE_DATA_PATH, \"test\")\n",
    "    IMAGE_VAL_PATH: str = os.path.join(IMAGE_DATA_PATH, \"val\")\n",
    "    IMAGE_TYPE: str = \"leftImg8bit\"\n",
    "    ANNOTATIONS_PREFIX: str = \"gtFine\"\n",
    "    SAVE_PATH: str = (\n",
    "        \"/home/hxastur/vscode-projects/cityscapes-segmentation/saved_models\"\n",
    "    )\n",
    "    batch_size: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 5\n",
    "    IMAGE_SIZE = (64, 128)\n",
    "    evalInterval = 1\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = \"cuda\" if torch.cuda.is_available else \"spu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c81a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 34\n",
    "\n",
    "CITYSCAPES_MASK_CLASSES = {\n",
    "    0: \"unlabeled\",\n",
    "    1: \"ego vehicle\",\n",
    "    2: \"rectification border\",\n",
    "    3: \"out of roi\",\n",
    "    4: \"static\",\n",
    "    5: \"dynamic\",\n",
    "    6: \"ground\",\n",
    "    7: \"road\",\n",
    "    8: \"sidewalk\",\n",
    "    9: \"parking\",\n",
    "    10: \"rail track\",\n",
    "    11: \"building\",\n",
    "    12: \"wall\",\n",
    "    13: \"fence\",\n",
    "    14: \"guard rail\",\n",
    "    15: \"bridge\",\n",
    "    16: \"tunnel\",\n",
    "    17: \"pole\",\n",
    "    18: \"polegroup\",\n",
    "    19: \"traffic light\",\n",
    "    20: \"traffic sign\",\n",
    "    21: \"vegetation\",\n",
    "    22: \"terrain\",\n",
    "    23: \"sky\",\n",
    "    24: \"person\",\n",
    "    25: \"rider\",\n",
    "    26: \"car\",\n",
    "    27: \"truck\",\n",
    "    28: \"bus\",\n",
    "    29: \"caravan\",\n",
    "    30: \"trailer\",\n",
    "    31: \"train\",\n",
    "    32: \"motorcycle\",\n",
    "    33: \"bicycle\",\n",
    "}\n",
    "\n",
    "CITYSCAPES_MASK_COLORS = {\n",
    "    0: (0, 0, 0),\n",
    "    1: (0, 0, 0),\n",
    "    2: (0, 0, 0),\n",
    "    3: (0, 0, 0),\n",
    "    4: (0, 0, 0),\n",
    "    5: (111, 74, 0),\n",
    "    6: (81, 0, 81),\n",
    "    7: (128, 64, 128),\n",
    "    8: (244, 35, 232),\n",
    "    9: (250, 170, 160),\n",
    "    10: (230, 150, 140),\n",
    "    11: (70, 70, 70),\n",
    "    12: (102, 102, 156),\n",
    "    13: (190, 153, 153),\n",
    "    14: (180, 165, 180),\n",
    "    15: (150, 100, 100),\n",
    "    16: (150, 120, 90),\n",
    "    17: (153, 153, 153),\n",
    "    18: (153, 153, 153),\n",
    "    19: (250, 170, 30),\n",
    "    20: (220, 220, 0),\n",
    "    21: (107, 142, 35),\n",
    "    22: (152, 251, 152),\n",
    "    23: (70, 130, 180),\n",
    "    24: (220, 20, 60),\n",
    "    25: (255, 0, 0),\n",
    "    26: (0, 0, 142),\n",
    "    27: (0, 0, 70),\n",
    "    28: (0, 60, 100),\n",
    "    29: (0, 0, 90),\n",
    "    30: (0, 0, 110),\n",
    "    31: (0, 80, 100),\n",
    "    32: (0, 0, 230),\n",
    "    33: (119, 11, 32),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442eeff",
   "metadata": {},
   "source": [
    "# Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2cc25",
   "metadata": {},
   "source": [
    "[Github Dataset Link](https://github.com/mcordts/cityscapesScripts)\n",
    "\n",
    "The folder structure of the Cityscapes dataset is as follows:\n",
    "\n",
    "**{root}/{type}{video}/{split}/{city}/{city}_{seq:0>6}_{frame:0>6}_{type}{ext}**\n",
    "\n",
    "The meaning of the individual elements is:\n",
    "\n",
    "**root** the root folder of the Cityscapes dataset. Many of our scripts check if an environment variable CITYSCAPES_DATASET pointing to this folder exists and use this as the default choice.\n",
    "\n",
    "**type** the type/modality of data, e.g. gtFine for fine ground truth, or leftImg8bit for left 8-bit images.\n",
    "\n",
    "**split** the split, i.e. train/val/test/train_extra/demoVideo. Note that not all kinds of data exist for all splits. Thus, do not be surprised to occasionally find empty folders.\n",
    "\n",
    "**city** the city in which this part of the dataset was recorded.\n",
    "\n",
    "**seq** the sequence number using 6 digits.\n",
    "\n",
    "**frame** the frame number using 6 digits. Note that in some cities very few, albeit very long sequences were recorded, while in some cities many short sequences were recorded, of which only the 19th frame is annotated.\n",
    "\n",
    "**ext** the extension of the file and optionally a suffix, e.g. _polygons.json for ground truth files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        IMAGE_DATA_PATH,\n",
    "        ANNOTATIONS_DATA_PATH,\n",
    "        ANNOTATIONS_TYPES,\n",
    "        ANNOTATIONS_PREFIX=\"_gtFine\",\n",
    "    ):\n",
    "        self.IMAGE_DATA_PATH = IMAGE_DATA_PATH\n",
    "        self.ANNOTATIONS_DATA_PATH = ANNOTATIONS_DATA_PATH\n",
    "        self.ANNOTATIONS_PREFIX = ANNOTATIONS_PREFIX\n",
    "        self.ANNOTATIONS_TYPES = ANNOTATIONS_TYPES\n",
    "\n",
    "    def get_images(self):\n",
    "        images = {}\n",
    "        cities = os.listdir(self.IMAGE_DATA_PATH)\n",
    "        for city in cities:\n",
    "            city_image_path = os.path.join(self.IMAGE_DATA_PATH, city)\n",
    "            files_image = os.listdir(city_image_path)\n",
    "            for file in files_image:\n",
    "                full_image_path = os.path.join(city_image_path, file)\n",
    "                splitted = file.split(\"_\")\n",
    "                if len(splitted) != 4:\n",
    "                    raise ValueError(\"Len of splitted != 4\")\n",
    "                image_type = \"left\"\n",
    "                image_city = splitted[0]\n",
    "                sequence_number = splitted[1]\n",
    "                frame_number = splitted[2]\n",
    "                image_name = f\"{image_city}_{sequence_number}_{frame_number}\"\n",
    "\n",
    "                image_arr = images.get(image_name, {})\n",
    "                image_arr.update({\"left\": full_image_path})\n",
    "                for ANNOTATION_TYPE in self.ANNOTATIONS_TYPES:\n",
    "                    annot_type = ANNOTATION_TYPE.split(\".\")[0]\n",
    "                    image_arr.update(\n",
    "                        {\n",
    "                            annot_type: os.path.join(\n",
    "                                self.ANNOTATIONS_DATA_PATH,\n",
    "                                f\"{image_city}/{image_name}{self.ANNOTATIONS_PREFIX}_{ANNOTATION_TYPE}\",\n",
    "                            )\n",
    "                        }\n",
    "                    )\n",
    "                images.update({image_name: image_arr})\n",
    "\n",
    "        for imgid in images.keys():\n",
    "            if len(images[imgid]) != 5:\n",
    "                raise ValueError(\"Len of arr %5 != 0\")\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152a76b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d328c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, images: dict, keys=None, size=(256, 512)):\n",
    "        self.images = images\n",
    "        if not keys:\n",
    "            self.keys = list(self.images.keys())\n",
    "        else:\n",
    "            self.keys = keys\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_index = self.keys[idx]\n",
    "        image_dict = self.images[image_index]\n",
    "\n",
    "        image_path = image_dict[\"left\"]\n",
    "        labelIds_path = image_dict[\"labelIds\"]\n",
    "\n",
    "        # mask_path = image_dict[\"color\"]\n",
    "        # instanceIds_path = image_dict[\"instanceIds\"]\n",
    "        # polygons_path = image_dict[\"polygons\"]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        mask = Image.open(labelIds_path)\n",
    "\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=self.size, interpolation=Image.NEAREST),\n",
    "            ]\n",
    "        )\n",
    "        image = transform(image)\n",
    "        mask = transform(mask)\n",
    "\n",
    "        mask_array = np.array(mask)\n",
    "        mask_tensor = torch.from_numpy(mask_array).long()\n",
    "        image_tensor = TF.to_tensor(image)  # C,H,W\n",
    "\n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    IMAGE_DATA_PATH=config.IMAGE_TRAIN_PATH,\n",
    "    ANNOTATIONS_DATA_PATH=config.ANNOTATIONS_TRAIN_PATH,\n",
    "    ANNOTATIONS_TYPES=config.ANNOTATION_TYPES,\n",
    ")\n",
    "images = processor.get_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204bbdf",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_splits(images: dict):\n",
    "    \"\"\"\n",
    "    Возвращает индексы train и test, которые передаются а датасет при создании\n",
    "    \"\"\"\n",
    "    keys_list = list(images.keys())\n",
    "    train_images, test_images = train_test_split(\n",
    "        keys_list, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return train_images, test_images\n",
    "\n",
    "\n",
    "train_images_idx, test_images_idx = get_index_splits(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images_idx), len(test_images_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7289611",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0dafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(CITYSCAPES_MASK_COLORS, images, val_index):\n",
    "    dataiter = iter(CityscapesDataset(images, keys=val_index))\n",
    "    image, mask = next(dataiter)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "    def decode_segmap(mask, colormap=CITYSCAPES_MASK_COLORS):\n",
    "        h, w = mask.shape\n",
    "        color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        for label in range(len(colormap)):\n",
    "            num_true = (mask == label).sum().item()\n",
    "            color_mask[mask == label] = colormap[label]\n",
    "        return color_mask\n",
    "\n",
    "    color_mask = decode_segmap(mask.numpy(), CITYSCAPES_MASK_COLORS)\n",
    "    blended = (0.5 * image.permute(1, 2, 0).numpy() + 0.5 * (color_mask / 255.0)).clip(\n",
    "        0, 1\n",
    "    )\n",
    "\n",
    "    axes[0].imshow(image.permute(1, 2, 0))\n",
    "    axes[1].imshow(blended)\n",
    "\n",
    "\n",
    "visualise(CITYSCAPES_MASK_COLORS, images, val_index=test_images_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcaf152",
   "metadata": {},
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batchnorm(self.cnn(x)))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_cnn,\n",
    "        pool=False,\n",
    "        upsample=False,\n",
    "        softmax=False,\n",
    "    ):\n",
    "        super(Block, self).__init__()\n",
    "        self.softmax = softmax\n",
    "        self.pool = pool\n",
    "        self.upsample = upsample\n",
    "        if num_cnn == 2:\n",
    "            self.block = nn.ModuleList(\n",
    "                [\n",
    "                    CNNBlock(in_channels=in_channels, out_channels=out_channels),\n",
    "                    CNNBlock(in_channels=out_channels, out_channels=out_channels),\n",
    "                ]\n",
    "            )\n",
    "        if num_cnn == 3:\n",
    "            self.block = nn.ModuleList(\n",
    "                [\n",
    "                    CNNBlock(in_channels=in_channels, out_channels=out_channels),\n",
    "                    CNNBlock(in_channels=out_channels, out_channels=out_channels),\n",
    "                    CNNBlock(in_channels=out_channels, out_channels=out_channels),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.mup = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, ind=None):\n",
    "\n",
    "        if self.upsample:\n",
    "            x = self.mup(x, ind)\n",
    "            # print(f\"UPSAMPLE {x.shape}\")\n",
    "\n",
    "        for module in self.block:\n",
    "            x = module(x)\n",
    "\n",
    "        if self.pool:\n",
    "            x, ind = self.mp(x)\n",
    "            # print(f\"POOL {x.shape}\")\n",
    "            return x, ind\n",
    "\n",
    "        if self.softmax:\n",
    "            x = self.sm(x)\n",
    "        # print(f\"ELSE {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=3, out_channels=32, num_two=2, num_blocks=5, channel_step=64\n",
    "    ):\n",
    "        super(SegNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.poolblock = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    Block(\n",
    "                        in_channels=(\n",
    "                            channel_step * 2 ** (i - 1) if i != 0 else in_channels\n",
    "                        ),\n",
    "                        out_channels=channel_step * 2**i,\n",
    "                        num_cnn=2 if i < num_two else 3,\n",
    "                        pool=True,\n",
    "                    )\n",
    "                )\n",
    "                for i in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.unpoolblock = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    in_channels=(channel_step * 2 ** (4 - i)),\n",
    "                    out_channels=(\n",
    "                        channel_step * 2 ** (3 - i)\n",
    "                        if i != num_blocks - 1\n",
    "                        else out_channels\n",
    "                    ),\n",
    "                    num_cnn=2 if i < num_two else 3,\n",
    "                    upsample=True,\n",
    "                    softmax=True if i == num_blocks - 1 else False,\n",
    "                    # softmax=False,\n",
    "                )\n",
    "                for i in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        index_list = []\n",
    "        # print(self.unpoolblock)\n",
    "        for module in self.poolblock:\n",
    "            x, ind = module(x)\n",
    "            index_list.append(ind)\n",
    "        for i, module in enumerate(self.unpoolblock):\n",
    "            ind = index_list[4 - i]\n",
    "            # print(x.shape, ind.shape)\n",
    "            x = module(x, ind)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290f12d",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        smooth = 1\n",
    "\n",
    "        target_one_hot = nn.functional.one_hot(\n",
    "            target.long(), num_classes=self.num_classes\n",
    "        )\n",
    "        target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()  # [B, C, H, W]\n",
    "\n",
    "        intersection = torch.sum(pred * target_one_hot, dim=(0, 2, 3))\n",
    "        pred_sum = torch.sum(pred, dim=(0, 2, 3))\n",
    "        target_sum = torch.sum(target_one_hot, dim=(0, 2, 3))\n",
    "\n",
    "        dice = 1 - ((2.0 * intersection + smooth) / (pred_sum + target_sum + smooth))\n",
    "        return dice.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bbc123",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debebca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, trainDataloader, testDataloader, evalInterval, savePath):\n",
    "        self.trainDataloader = trainDataloader\n",
    "        self.testDataloader = testDataloader\n",
    "        self.evalInterval = evalInterval\n",
    "        self.savePath = savePath\n",
    "\n",
    "    def train(self, net, optimizer, epochs, criterion):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for i, (batch_image, batch_mask) in tqdm(\n",
    "                enumerate(self.trainDataloader), total=len(self.trainDataloader)\n",
    "            ):\n",
    "                batch_image, batch_mask = batch_image.to(device), batch_mask.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = net(batch_image)\n",
    "\n",
    "                loss = criterion(output, batch_mask)\n",
    "                print(f\"loss: {loss}\")\n",
    "                loss.backward()\n",
    "                epoch_loss += loss\n",
    "                optimizer.step()\n",
    "            epoch_loss /= len(self.trainDataloader)\n",
    "            print(f\"train loss: {epoch_loss}\")\n",
    "\n",
    "            if (epoch + 1) % self.evalInterval == 0:\n",
    "                self.test()\n",
    "\n",
    "    def test(self, net):\n",
    "        with torch.no_grad():\n",
    "            for images, masks in self.testLoader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = net(images)\n",
    "\n",
    "    def save_model(self, model):\n",
    "        filename = \"model\"\n",
    "        filepath = os.path.join(self.savePath, filename)\n",
    "        torch.save(model, filepath)\n",
    "        print(f\"Saved model with name: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b0b75",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486abac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    IMAGE_DATA_PATH=config.IMAGE_TRAIN_PATH,\n",
    "    ANNOTATIONS_DATA_PATH=config.ANNOTATIONS_TRAIN_PATH,\n",
    "    ANNOTATIONS_TYPES=config.ANNOTATION_TYPES,\n",
    ")\n",
    "images = processor.get_images()\n",
    "train_images_idx, test_images_idx = get_index_splits(images)\n",
    "\n",
    "trainDataset = CityscapesDataset(images, train_images_idx)\n",
    "testDataset = CityscapesDataset(images, test_images_idx)\n",
    "\n",
    "trainDataloader = DataLoader(dataset=trainDataset, batch_size=config.batch_size)\n",
    "testDataloader = DataLoader(dataset=testDataset, batch_size=1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainDataloader=trainDataloader,\n",
    "    testDataloader=testDataloader,\n",
    "    evalInterval=config.evalInterval,\n",
    "    savePath=config.SAVE_PATH,\n",
    ")\n",
    "\n",
    "net = SegNet(in_channels=3, out_channels=NUM_CLASSES).to(device)\n",
    "optimizer = Adam(net.parameters(), lr=config.learning_rate)\n",
    "criterion = DiceLoss(NUM_CLASSES)\n",
    "\n",
    "trainer.train(net=net, optimizer=optimizer, epochs=config.epochs, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = Processor(\n",
    "#     IMAGE_DATA_PATH=config.IMAGE_TRAIN_PATH,\n",
    "#     ANNOTATIONS_DATA_PATH=config.ANNOTATIONS_TRAIN_PATH,\n",
    "#     ANNOTATIONS_TYPES=config.ANNOTATION_TYPES,\n",
    "# )\n",
    "# dataset = CityscapesDataset(processor)\n",
    "# dataloader = DataLoader(dataset=dataset, batch_size=config.batch_size)\n",
    "# data = next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cityscapes-segmentation-py3.12 (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
